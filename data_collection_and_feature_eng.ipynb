{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d4d62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\anaconda3\\envs\\ox311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library\n",
    "import bisect\n",
    "import datetime\n",
    "import gc\n",
    "import logging\n",
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from textwrap import dedent\n",
    "from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "# Third-party\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit, prange\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tqdm.auto import tqdm\n",
    "import wrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = \"1995-01-01\" # TODO: CHANGE\n",
    "END_DATE = '2024-12-31'\n",
    "USERNAME = 'your_wrds_username'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e367e32c",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2810de",
   "metadata": {},
   "source": [
    "## Data collection and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60899ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "\n",
    "class WRDSDataCollector:\n",
    "    \"\"\"WRDS/CRSP data collector for monthly and daily pulls.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        username: str = USERNAME,\n",
    "        data_dir: str = \"./data\",\n",
    "        wrds_conn: Optional[wrds.Connection] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize the collector.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        username : str\n",
    "            WRDS user ID.\n",
    "        data_dir : str\n",
    "            Root folder where pulled CSV/Parquet files will be written.\n",
    "        wrds_conn : Optional[wrds.Connection]\n",
    "            If provided, reuse this WRDS connection.\n",
    "        \"\"\"\n",
    "        self.conn = wrds_conn or wrds.Connection(wrds_username=username)\n",
    "        self.data_dir = Path(data_dir)\n",
    "        (self.data_dir / \"monthly\").mkdir(parents=True, exist_ok=True)\n",
    "        (self.data_dir / \"daily\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        logger.info(f\"Connected to WRDS as {username}\")\n",
    "        logger.info(f\"Data will be saved under {self.data_dir.resolve()}\")\n",
    "\n",
    "    def fetch_stock_universe(\n",
    "        self,\n",
    "        date: str,\n",
    "        min_market_cap: float = 1e9,\n",
    "        exchange_codes: List[int] = [1, 2, 3],\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Universe of common stocks on selected exchanges at a given date.\n",
    "\n",
    "        Market cap is computed as ``shrout * abs(prc) * 1000``.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        date : str\n",
    "            Date in \"YYYY-MM-DD\" format.\n",
    "        min_market_cap : float, default 1e9\n",
    "            Minimum market cap in USD.\n",
    "        exchange_codes : List[int]\n",
    "            Exchanges to include (1=NYSE, 2=AMEX, 3=NASDAQ).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Columns: permno, ticker, comnam, market_cap, shrcd, exchcd, siccd.\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT\n",
    "          dsf.permno,\n",
    "          sn.ticker,\n",
    "          sn.comnam,\n",
    "          (dsf.shrout * ABS(dsf.prc) * 1000) AS market_cap,\n",
    "          sn.shrcd,\n",
    "          sn.exchcd,\n",
    "          sn.siccd\n",
    "        FROM crsp.dsf AS dsf\n",
    "        JOIN crsp.stocknames AS sn\n",
    "          ON dsf.permno = sn.permno\n",
    "         AND sn.namedt <= dsf.date\n",
    "         AND (sn.nameenddt >= dsf.date OR sn.nameenddt IS NULL)\n",
    "        WHERE dsf.date = '{date}'\n",
    "          AND (dsf.shrout * ABS(dsf.prc) * 1000) >= {min_market_cap}\n",
    "          AND sn.exchcd IN ({','.join(map(str, exchange_codes))})\n",
    "          AND sn.shrcd IN (10, 11)\n",
    "        ORDER BY market_cap DESC\n",
    "        \"\"\"\n",
    "        return self.conn.raw_sql(query)\n",
    "\n",
    "    def fetch_returns_matrix(\n",
    "        self,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Daily total return matrix (T x N) for common shares on NYSE/AMEX/NASDAQ.\n",
    "\n",
    "        ``total_ret = ret + coalesce(dlret, 0)``\n",
    "        \"\"\"\n",
    "        sql = f\"\"\"\n",
    "            SELECT\n",
    "            dsf.permno,\n",
    "            dsf.date,\n",
    "            dsf.ret + COALESCE(d.dlret, 0) AS total_ret\n",
    "            FROM crsp.dsf AS dsf\n",
    "            JOIN crsp.stocknames AS sn\n",
    "              ON dsf.permno = sn.permno\n",
    "             AND sn.namedt <= dsf.date\n",
    "             AND (sn.nameenddt >= dsf.date OR sn.nameenddt IS NULL)\n",
    "            LEFT JOIN crsp.dsedelist AS d\n",
    "              ON dsf.permno = d.permno\n",
    "             AND dsf.date = d.dlstdt  -- non-null only on a delisting day\n",
    "            WHERE dsf.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "              AND sn.exchcd IN (1, 2, 3)\n",
    "              AND sn.shrcd IN (10, 11)\n",
    "        \"\"\"\n",
    "        # Pull into a long DataFrame.\n",
    "        df = self.conn.raw_sql(sql, date_cols=['date'])\n",
    "\n",
    "        # Pivot to wide: index=dates, columns=permnos, values=total_ret.\n",
    "        ret_mat = (\n",
    "            df.pivot(index='date', columns='permno', values='total_ret')\n",
    "              .sort_index()\n",
    "        )\n",
    "        ret_mat.index.name = 'date'\n",
    "        ret_mat.columns.name = 'permno'\n",
    "        return ret_mat\n",
    "\n",
    "    def fetch_mkt_cap(self, start_date: str, end_date: str):\n",
    "        sql = f\"\"\"\n",
    "            SELECT\n",
    "                permno,\n",
    "                date,\n",
    "                (shrout * ABS(prc) * 1000) AS market_cap\n",
    "            FROM crsp.dsf\n",
    "            WHERE date BETWEEN '{start_date}' AND '{end_date}'\n",
    "              AND shrout IS NOT NULL\n",
    "              AND prc IS NOT NULL\n",
    "        \"\"\"\n",
    "        mcap = self.conn.raw_sql(sql, date_cols=['date'])\n",
    "        return mcap\n",
    "\n",
    "    def fetch_split_adj_close(\n",
    "        self,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "    ) -> pd.DataFrame:\n",
    "        sql = f\"\"\"\n",
    "            SELECT\n",
    "                dsf.date,\n",
    "                dsf.permno,\n",
    "                ABS(dsf.prc) / NULLIF(dsf.cfacpr, 0) AS adj_close\n",
    "            FROM crsp.dsf AS dsf\n",
    "            JOIN crsp.stocknames AS sn\n",
    "              ON dsf.permno = sn.permno\n",
    "             AND sn.namedt <= dsf.date\n",
    "             AND (sn.nameenddt >= dsf.date OR sn.nameenddt IS NULL)\n",
    "            WHERE dsf.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "              AND sn.exchcd IN (1, 2, 3)\n",
    "              AND sn.shrcd IN (10, 11)\n",
    "        \"\"\"\n",
    "        df = self.conn.raw_sql(sql, date_cols=['date'])\n",
    "        price_mat = df.pivot(index='date', columns='permno', values='adj_close').sort_index()\n",
    "        price_mat.index.name = 'date'\n",
    "        price_mat.columns.name = 'permno'\n",
    "        return price_mat\n",
    "\n",
    "    def fetch_etf_time_series(\n",
    "        self,\n",
    "        etf_tickers: List[str],\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Daily total returns for a list of ETF tickers.\n",
    "\n",
    "        If a ticker maps to multiple PERMNOs over time, the most recent name\n",
    "        record per date is used.\n",
    "        \"\"\"\n",
    "        if not self.conn:\n",
    "            print(\"No WRDS connection available.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        tickers_list_str = \"','\".join(etf_tickers)\n",
    "\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            a.date, \n",
    "            b.ticker,\n",
    "            a.ret AS daily_ret,\n",
    "            COALESCE(d.dlret, 0) AS delist_ret,\n",
    "            CASE \n",
    "                WHEN d.dlret IS NOT NULL THEN d.dlret\n",
    "                ELSE a.ret\n",
    "            END AS total_ret\n",
    "        FROM crsp.dsf AS a\n",
    "        INNER JOIN crsp.stocknames AS b \n",
    "            ON a.permno = b.permno\n",
    "        LEFT JOIN crsp.dsedelist AS d \n",
    "            ON a.permno = d.permno AND a.date = d.dlstdt\n",
    "        WHERE \n",
    "            b.ticker IN ('{tickers_list_str}')\n",
    "            AND a.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "            AND b.namedt <= a.date \n",
    "            AND a.date <= b.nameenddt\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            df = self.conn.raw_sql(sql_query, date_cols=['date'])\n",
    "\n",
    "            # Pivot to get tickers as columns.\n",
    "            pivot_df = df.pivot_table(\n",
    "                index='date',\n",
    "                columns='ticker',\n",
    "                values='total_ret',\n",
    "                aggfunc='first',  # use first to handle potential duplicates\n",
    "            )\n",
    "\n",
    "            # Reorder columns to match input list and sort by date.\n",
    "            pivot_df = pivot_df.reindex(columns=etf_tickers).sort_index()\n",
    "\n",
    "            return pivot_df\n",
    "\n",
    "        except Exception as e:  # pragma: no cover - pass-through logging\n",
    "            print(f\"An error occurred while fetching ETF data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def fetch_dollar_turnover(\n",
    "        self,\n",
    "        start_date: str,\n",
    "        end_date: str,\n",
    "        universe_filters: bool = True,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Daily split-consistent dollar turnover.\n",
    "\n",
    "        Definitions\n",
    "        -----------\n",
    "        vol_adj = abs(prc) * (vol * cfacshr)\n",
    "        mkt_cap = shrout * abs(prc) * 1000  # shrout is in thousands\n",
    "        turnover = vol_adj / mkt_cap\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        start_date, end_date : str\n",
    "            'YYYY-MM-DD'.\n",
    "        universe_filters : bool, default True\n",
    "            If True, restrict to shrcd 10/11 and exchcd 1/2/3 via stocknames.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Columns: permno, date, dollar_vol, market_cap, turnover.\n",
    "        \"\"\"\n",
    "        if universe_filters:\n",
    "            join_where = f\"\"\"\n",
    "            JOIN crsp.stocknames sn\n",
    "              ON dsf.permno = sn.permno\n",
    "             AND sn.namedt <= dsf.date\n",
    "             AND (sn.nameenddt >= dsf.date OR sn.nameenddt IS NULL)\n",
    "            WHERE dsf.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "              AND sn.shrcd IN (10, 11)\n",
    "              AND sn.exchcd IN (1, 2, 3)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            join_where = f\"\"\"\n",
    "            WHERE dsf.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "            \"\"\"\n",
    "\n",
    "        sql = dedent(\n",
    "            f\"\"\"\n",
    "        WITH base AS (\n",
    "            SELECT\n",
    "                dsf.permno,\n",
    "                dsf.date,\n",
    "                ABS(dsf.prc) AS prc,\n",
    "                dsf.vol AS vol,\n",
    "                dsf.shrout AS shrout,\n",
    "                dsf.cfacshr AS cfacshr\n",
    "            FROM crsp.dsf dsf\n",
    "            {join_where}\n",
    "              AND dsf.prc IS NOT NULL\n",
    "              AND dsf.vol IS NOT NULL\n",
    "              AND dsf.shrout IS NOT NULL\n",
    "              AND dsf.cfacshr IS NOT NULL\n",
    "        )\n",
    "        SELECT\n",
    "            permno,\n",
    "            date,\n",
    "            cfacshr,\n",
    "            prc * (vol) AS dollar_vol,\n",
    "            shrout * prc * 1000 AS market_cap,\n",
    "            (prc * (vol)) / NULLIF(shrout * prc * 1000, 0) AS turnover\n",
    "        FROM base\n",
    "        ORDER BY date, permno\n",
    "        \"\"\"\n",
    "        )\n",
    "\n",
    "        return self.conn.raw_sql(sql, date_cols=['date'])\n",
    "\n",
    "    def fetch_permco_map(self, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "        sql = f\"\"\"\n",
    "            SELECT\n",
    "                dsf.date,\n",
    "                dsf.permno,\n",
    "                sn.permco\n",
    "            FROM crsp.dsf dsf\n",
    "            JOIN crsp.stocknames sn\n",
    "              ON dsf.permno = sn.permno\n",
    "             AND sn.namedt <= dsf.date\n",
    "             AND (sn.nameenddt >= dsf.date OR sn.nameenddt IS NULL)\n",
    "            WHERE dsf.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "              AND sn.shrcd IN (10, 11)\n",
    "              AND sn.exchcd IN (1, 2, 3)\n",
    "        \"\"\"\n",
    "        return self.conn.raw_sql(sql, date_cols=['date'])\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the WRDS connection.\"\"\"\n",
    "        self.conn.close()\n",
    "        logger.info(\"WRDS connection closed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca377a",
   "metadata": {},
   "source": [
    "Fetching from WRDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e514bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 20:26:34,501 - INFO - Connected to WRDS as benjaminlhr555\n",
      "2025-07-23 20:26:34,502 - INFO - Data will be saved under C:\\Users\\benja\\Desktop\\oxford\\Dissertation\\cs_strats\\data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 20:57:39,600 - INFO - WRDS connection closed\n"
     ]
    }
   ],
   "source": [
    "ETF_TICKERS = ['SPY', 'MTUM']\n",
    "\n",
    "wrds_data_collector = WRDSDataCollector(username=USERNAME)\n",
    "etf_returns_matrix = wrds_data_collector.fetch_etf_time_series(etf_tickers=ETF_TICKERS, start_date=START_DATE, end_date=END_DATE)\n",
    "returns_matrix = wrds_data_collector.fetch_returns_matrix(start_date=START_DATE, end_date=END_DATE)\n",
    "mcap = wrds_data_collector.fetch_mkt_cap(start_date=START_DATE, end_date=END_DATE)\n",
    "split_adjusted_close = wrds_data_collector.fetch_split_adj_close(start_date=START_DATE, end_date=END_DATE)\n",
    "turnover_df = wrds_data_collector.fetch_dollar_turnover(start_date=START_DATE, end_date=END_DATE, universe_filters=True)\n",
    "wrds_data_collector.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5baf38",
   "metadata": {},
   "source": [
    "Saving files locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3c305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_matrix.to_csv('data/returns_matrix.csv', index_label='date')\n",
    "mcap.to_csv('data/mcap.csv', index=False)\n",
    "split_adjusted_close.to_csv('data/split_adjusted_close.csv', index_label='date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63254232",
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_df.astype({c: 'float64' for c in turnover_df.select_dtypes(include=['Int64','Float64']).columns}).to_hdf('data/turnover_df.h5', key='turnover_df', format='fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e41bb6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_returns_matrix.astype({c: 'float64' for c in etf_returns_matrix.select_dtypes(include=['Int64','Float64']).columns}).to_hdf('data/etf_returns_matrix.h5', key='etf_returns_matrix', format='fixed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23993fd8",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c690d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_matrix = pd.read_csv(\n",
    "    'data/returns_matrix.csv',\n",
    "    index_col='date',\n",
    "    parse_dates=['date']\n",
    ")\n",
    "\n",
    "mcap = pd.read_csv('data/mcap.csv')\n",
    "mcap['date'] = pd.to_datetime(mcap['date'])\n",
    "\n",
    "split_adjusted_close = pd.read_csv(\n",
    "    'data/split_adjusted_close.csv',\n",
    "    index_col='date',\n",
    "    parse_dates=['date']\n",
    ")\n",
    "\n",
    "returns_matrix.columns = returns_matrix.columns.astype(int)\n",
    "split_adjusted_close.columns = split_adjusted_close.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b5aae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_matrix = pd.read_csv(\n",
    "    'data/returns_matrix.csv',\n",
    "    index_col='date',\n",
    "    parse_dates=['date']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68daf3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_matrix.columns = returns_matrix.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f8a0ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_df = pd.read_hdf(\"data/turnover_df.h5\", key=\"turnover_df\")\n",
    "turnover_df['permno'] = turnover_df['permno'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4167a",
   "metadata": {},
   "source": [
    "# Initializing Mask for Stocks to Trade\n",
    "It will continue to be updated along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bac69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STOCKS_TO_TRADE = 1000\n",
    "\n",
    "valid253 = (\n",
    "    returns_matrix.notna()                         # bool matrix\n",
    "                 .rolling(253, min_periods=253)    # look-back window\n",
    "                 .sum() == 253                     # True ⇔ all 253 days present\n",
    ")\n",
    "\n",
    "mcap_wide = mcap.pivot(index='date', columns='permno', values='market_cap')\n",
    "mcap_wide_aligned = mcap_wide.reindex_like(returns_matrix)\n",
    "mcap_wide_where_valid253 = mcap_wide_aligned.where(valid253)\n",
    "mask_for_stocks_to_trade = (mcap_wide_where_valid253.rank(axis=1, method='first', ascending=False) <= NUM_STOCKS_TO_TRADE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358adfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment and run this chunk to save memory (if needed) since we don't need these variables anymore\n",
    "# del mcap\n",
    "# del mcap_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d640b2",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b2149",
   "metadata": {},
   "source": [
    "## Volume Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc9e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_turnover_rolling_avg(\n",
    "    df: pd.DataFrame,\n",
    "    lookbacks: Iterable[int] = (21, 63, 126, 252),\n",
    "    coverage: float = 0.8,\n",
    "    permno_col: str = \"permno\",\n",
    "    date_col: str = \"date\",\n",
    "    turnover_col: str = \"turnover\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add rolling-average turnover columns for multiple lookback windows.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Input frame containing at least [permno_col, date_col, turnover_col].\n",
    "    lookbacks : iterable of int\n",
    "        Window lengths (in trading days) to compute.\n",
    "    coverage : float\n",
    "        Minimum fraction of non-null observations required in the window.\n",
    "    permno_col, date_col, turnover_col : str\n",
    "        Column names.\n",
    "    copy : bool\n",
    "        If True, work on a copy; otherwise modify in place.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        DataFrame with new columns: `avg_turnover_of_past_{lb}_days`.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure proper ordering\n",
    "    df.sort_values([permno_col, date_col], inplace=True)\n",
    "\n",
    "    grp = df.groupby(permno_col)[turnover_col]\n",
    "\n",
    "    for lb in lookbacks:\n",
    "        minp = int(np.floor(coverage * lb))\n",
    "        colname = f\"avg_turnover_of_past_{lb}_days\"\n",
    "        df[colname] = (\n",
    "        turnover_df\n",
    "            .groupby('permno')['turnover']\n",
    "            .apply(lambda s: s.shift(1)         # skip “today”\n",
    "                                .rolling(window=lb, min_periods=minp)\n",
    "                                .mean())\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_turnover_normalized_betas(\n",
    "    df: pd.DataFrame,\n",
    "    lookbacks: Iterable[int] = (21, 63, 126, 252),\n",
    "    coverage: float = 0.8,\n",
    "    permno_col: str = \"permno\",\n",
    "    date_col: str = \"date\",\n",
    "    turnover_col: str = \"turnover\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add raw/normalized beta & R^2 of OLS regression: turnover ~ trading_day_index\n",
    "    using the previous lookback trading days for each (permno, date).\n",
    "\n",
    "    Normalization: beta / median(turnover over previous lookback days).\n",
    "\n",
    "    For memory‑efficiency: iterates over contiguous slices of each permno (no pandas groupby).\n",
    "    Uses float64 for numerical stability. Rolling median computed with a\n",
    "    sliding sorted list (O(n * lookback)) which is acceptable for typical\n",
    "    lookbacks (<=252).\n",
    "    \"\"\"\n",
    "    # Sort once\n",
    "    df.sort_values([permno_col, date_col], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    permno_vals = df[permno_col].to_numpy()\n",
    "    y_all = df[turnover_col].to_numpy(dtype=np.float64)\n",
    "    N = len(df)\n",
    "\n",
    "    # Find group boundaries (assumes sorted by permno)\n",
    "    change = np.flatnonzero(permno_vals[1:] != permno_vals[:-1]) + 1\n",
    "    starts = np.concatenate(([0], change))\n",
    "    ends = np.concatenate((change, [N]))\n",
    "\n",
    "    for lb in lookbacks:\n",
    "        beta_raw_arr = np.full(N, np.nan, dtype=np.float64)\n",
    "        beta_norm_arr = np.full(N, np.nan, dtype=np.float64)\n",
    "        r2_arr = np.full(N, np.nan, dtype=np.float64)\n",
    "        minp = int(np.floor(lb * coverage))\n",
    "\n",
    "        for start, end in zip(starts, ends):\n",
    "            n = end - start\n",
    "            if n <= lb:\n",
    "                continue  # no row has enough history\n",
    "\n",
    "            y = y_all[start:end]                 # view\n",
    "            x = np.arange(n, dtype=np.float64)   # day index within permno\n",
    "            valid = ~np.isnan(y)\n",
    "\n",
    "            #  rolling median of previous lb days \n",
    "            median_arr = np.full(n, np.nan, dtype=np.float64)\n",
    "            # initialize sorted window with first lb observations (previous days for j=lb)\n",
    "            window_vals = [val for val in y[0:lb] if not np.isnan(val)]\n",
    "            window_vals.sort()\n",
    "\n",
    "            def current_median(vals):\n",
    "                m = len(vals)\n",
    "                if m == 0:\n",
    "                    return np.nan\n",
    "                if m % 2:\n",
    "                    return vals[m // 2]\n",
    "                else:\n",
    "                    return 0.5 * (vals[m // 2 - 1] + vals[m // 2])\n",
    "\n",
    "            # j runs from lb .. n-1\n",
    "            for j in range(lb, n):\n",
    "                if len(window_vals) >= minp:\n",
    "                    median_arr[j] = current_median(window_vals)\n",
    "                # slide window: remove outgoing y[j-lb], add incoming y[j]\n",
    "                out_val = y[j - lb]\n",
    "                if not np.isnan(out_val):\n",
    "                    pos = bisect.bisect_left(window_vals, out_val)\n",
    "                    # safety check (should exist)\n",
    "                    if pos < len(window_vals) and window_vals[pos] == out_val:\n",
    "                        window_vals.pop(pos)\n",
    "                in_val = y[j]\n",
    "                if not np.isnan(in_val):\n",
    "                    bisect.insort(window_vals, in_val)\n",
    "\n",
    "            # OLS slope/R^2 using prefix sums \n",
    "            y_valid = np.where(valid, y, 0.0)\n",
    "            x_valid = np.where(valid, x, 0.0)\n",
    "\n",
    "            cs_y  = np.empty(n + 1); cs_y[0] = 0;  cs_y[1:]  = np.cumsum(y_valid)\n",
    "            cs_y2 = np.empty(n + 1); cs_y2[0] = 0; cs_y2[1:] = np.cumsum(y_valid * y_valid)\n",
    "            cs_x  = np.empty(n + 1); cs_x[0] = 0;  cs_x[1:]  = np.cumsum(x_valid)\n",
    "            cs_x2 = np.empty(n + 1); cs_x2[0] = 0; cs_x2[1:] = np.cumsum(np.where(valid, x * x, 0.0))\n",
    "            cs_xy = np.empty(n + 1); cs_xy[0] = 0; cs_xy[1:] = np.cumsum(np.where(valid, x * y, 0.0))\n",
    "            cs_n  = np.empty(n + 1, dtype=np.int64); cs_n[0] = 0; cs_n[1:]  = np.cumsum(valid.astype(np.int64))\n",
    "\n",
    "            j = np.arange(lb, n, dtype=np.int64)\n",
    "            S_y  = cs_y[j]  - cs_y[j - lb]\n",
    "            S_y2 = cs_y2[j] - cs_y2[j - lb]\n",
    "            S_x  = cs_x[j]  - cs_x[j - lb]\n",
    "            S_x2 = cs_x2[j] - cs_x2[j - lb]\n",
    "            S_xy = cs_xy[j] - cs_xy[j - lb]\n",
    "            n_valid = cs_n[j] - cs_n[j - lb]\n",
    "\n",
    "            numer_beta = n_valid * S_xy - S_x * S_y\n",
    "            denom_beta = n_valid * S_x2 - S_x * S_x\n",
    "            S_yc2 = n_valid * S_y2 - S_y * S_y\n",
    "\n",
    "            with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "                beta_raw = numer_beta / denom_beta\n",
    "                r2 = (numer_beta * numer_beta) / (denom_beta * S_yc2)\n",
    "\n",
    "            # Valid slope mask\n",
    "            mask = (n_valid >= minp) & (denom_beta > 0) & (S_yc2 > 0)\n",
    "            rows_global = start + j[mask]\n",
    "\n",
    "            # Raw beta\n",
    "            beta_raw_arr[rows_global] = beta_raw[mask]\n",
    "            r2_arr[rows_global] = r2[mask]\n",
    "\n",
    "            # Normalized beta: divide by median (must have median, nonzero)\n",
    "            med_vals = median_arr[j[mask]]\n",
    "            norm_mask = ~np.isnan(med_vals) & (med_vals != 0)\n",
    "            if norm_mask.any():\n",
    "                beta_norm_arr[rows_global[norm_mask]] = (\n",
    "                    beta_raw[mask][norm_mask] / med_vals[norm_mask]\n",
    "                )\n",
    "\n",
    "        # Attach columns\n",
    "        # df[f\"beta_turnover_wrt_day_{lb}_days_raw\"] = beta_raw_arr\n",
    "        df[f\"beta_turnover_wrt_day_{lb}_days\"] = beta_norm_arr\n",
    "        df[f\"r2_turnover_wrt_day_{lb}_days\"] = r2_arr\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94efa8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_lookbacks = [21, 63, 126, 252]\n",
    "turnover_df = turnover_df.sort_values(['permno', 'date'])\n",
    "\n",
    "turnover_df = add_turnover_rolling_avg(turnover_df, lookbacks=turnover_lookbacks, \n",
    "                                       coverage=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c6e7eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_df = add_turnover_normalized_betas(df=turnover_df, lookbacks=turnover_lookbacks, coverage=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c729b886",
   "metadata": {},
   "source": [
    "### Convert to wide format (so that we can update mask_for_stocks_to_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88a46002",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_turnover_dict = {}\n",
    "avg_turnover_cols = [c for c in turnover_df.columns if 'avg_turnover_of_past' in c]\n",
    "assert len(avg_turnover_cols) > 0\n",
    "for col in avg_turnover_cols:\n",
    "    avg_turnover_dict[col] = (turnover_df.groupby(['date', 'permno'])[col].mean()\n",
    "        .unstack('permno')\n",
    "        .sort_index()\n",
    "        .sort_index(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f310bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_reg_dict = {}\n",
    "turnover_reg_cols = [c for c in turnover_df.columns if \"beta_turnover_wrt_day_\" in c] + [c for c in turnover_df.columns if \"r2_turnover_wrt_day_\" in c]\n",
    "assert len(turnover_reg_cols) > 0\n",
    "for col in turnover_reg_cols:\n",
    "    turnover_reg_dict[col] = (turnover_df.groupby(['date', 'permno'])[col].mean()\n",
    "        .unstack('permno')\n",
    "        .sort_index()\n",
    "        .sort_index(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda09821",
   "metadata": {},
   "source": [
    "Save turnover_reg_dict, avg_turnover_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c83db872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('features/turnover_reg_dict.pkl','wb') as f:\n",
    "    pickle.dump(turnover_reg_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "072daf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('features/avg_turnover_dict.pkl','wb') as f:\n",
    "    pickle.dump(avg_turnover_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d89307b",
   "metadata": {},
   "source": [
    "Load turnover_reg_dict, avg_turnover_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features/avg_turnover_dict.pkl','rb') as f:\n",
    "    avg_turnover_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features/avg_turnover_dict.pkl','rb') as f:\n",
    "    avg_turnover_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bbc070",
   "metadata": {},
   "source": [
    "## Raw returns and vol-scaled returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37115d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewma_std_strict_window_size(\n",
    "    returns_matrix: pd.DataFrame,\n",
    "    span: int = 63,\n",
    "    window: int = 253\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute an exponentially-weighted std over exactly the last `window` days.\n",
    "    If any of the past `window` returns are NaN, the result is NaN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns_matrix : pd.DataFrame\n",
    "        Daily total-return matrix (dates × permnos)\n",
    "    span : int\n",
    "        EW span for decay (alpha = 2/(span+1)).\n",
    "    window : int\n",
    "        Number of days to include (exactly) in each vol estimate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vol : DataFrame\n",
    "        Same shape as `mat`, but the first `window-1` rows (and\n",
    "        any window containing NaNs) will be NaN.\n",
    "    \"\"\"\n",
    "    α = 2.0 / (span + 1)\n",
    "    # raw weights, newest day last\n",
    "    raw_w = (1 - α) ** np.arange(window)[::-1]\n",
    "    S1 = raw_w.sum()\n",
    "    S2 = (raw_w**2).sum()\n",
    "    # bias-correction factor for variance (ddof=1 analogue)\n",
    "    bias_var = S1**2 / (S1**2 - S2)\n",
    "    sqrt_bias = np.sqrt(bias_var)\n",
    "\n",
    "    # normalized weights for the population formula\n",
    "    w = raw_w / S1\n",
    "\n",
    "    def _wstd(x: np.ndarray) -> float:\n",
    "        # x is a length-`window` slice of returns\n",
    "        if x.shape[0] < window or np.isnan(x).any():\n",
    "            return np.nan\n",
    "        μ = np.dot(w, x)\n",
    "        pop_std = np.sqrt(np.dot(w, (x - μ) ** 2))\n",
    "        # apply the same bias correction pandas uses:\n",
    "        return sqrt_bias * pop_std\n",
    "\n",
    "    return (\n",
    "        returns_matrix\n",
    "        .rolling(window=window, min_periods=window)\n",
    "        .apply(_wstd, raw=True)\n",
    "    )\n",
    "    \n",
    "def calc_vol_and_time_scaled_kday_returns(\n",
    "    ret_dict: Dict[int, pd.DataFrame],\n",
    "    vol_mat: pd.DataFrame\n",
    ") -> Dict[int, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Normalize each k-day return matrix by the ex-ante volatility.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ret_dict : dict[int, DataFrame]\n",
    "        Mapping lookback k → raw k-day return matrix (dates × assets).\n",
    "    vol_mat : DataFrame\n",
    "        Ex-ante volatility σ_t (dates × assets), e.g. from `calc_ewm_vol`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scaled_dict : dict[int, DataFrame]\n",
    "        Same keys as `ret_dict`, but each DataFrame is elementwise\n",
    "        divided by `vol_mat`, giving vol-scaled returns, then further divided by sqrt(k) to make the values comparable against\n",
    "        different horizons\n",
    "    \"\"\"\n",
    "    scaled = {}\n",
    "    for k, ret in ret_dict.items():\n",
    "        length = None\n",
    "        if isinstance(k, int):\n",
    "            length = k\n",
    "        elif isinstance(k, tuple) and len(k) == 2:\n",
    "            length = k[0] - k[1]\n",
    "        else:\n",
    "            raise Exception('Key for ret_dict should be either an integer or a length 2 tuple')\n",
    "        # align on dates/assets in case of mismatches\n",
    "        ret_aligned = ret.reindex_like(vol_mat)\n",
    "        scaled[k] = ret_aligned.div(vol_mat).div(np.sqrt(length))\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a2e3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "WindowSpec = Union[int, Tuple[int, int]]\n",
    "\n",
    "\n",
    "def calc_kday_returns(\n",
    "    returns_matrix: pd.DataFrame,\n",
    "    windows: Iterable[WindowSpec]\n",
    ") -> Dict[WindowSpec, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Compute total return matrices for multiple look-back specifications.\n",
    "\n",
    "    For an integer window k:\n",
    "        r_t^(k) = exp( sum_{i=0..k-1} log(1 + r_{t-i}) ) - 1\n",
    "      i.e. k-day total return ending at date t (includes day t).\n",
    "\n",
    "    For a tuple window (a, b) with a > b >= 0:\n",
    "        Return from *a days ago* up to and including *b days ago*, skipping\n",
    "        the most recent b days. Length of interval = a - b trading days.\n",
    "        Formally (aligned to date t):\n",
    "            r_t^(a,b) = exp( sum_{i=b .. a-1} log(1 + r_{t-i}) ) - 1\n",
    "        (Note: i=0 corresponds to day t.)\n",
    "\n",
    "        Example: (252, 21) sums log-returns for days t-252+1 ... t-21 (length 231),\n",
    "        i.e. from 252 days ago through 21 days ago inclusive, excluding the last\n",
    "        21 days.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns_matrix : pd.DataFrame\n",
    "        Daily simple returns (dates × assets), each entry r_t.\n",
    "    windows : iterable of int or (int, int)\n",
    "        Look-back specs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[WindowSpec, pd.DataFrame]\n",
    "        Maps each window spec to a DataFrame of total returns.\n",
    "        Keys are the same objects (ints or tuples) provided in `windows`.\n",
    "    \"\"\"\n",
    "    # Sanitize returns to avoid log(<=0) issues: r <= -1 ⇒ NaN\n",
    "    safe = returns_matrix.where(returns_matrix > -1)\n",
    "    log_ret = np.log1p(safe)\n",
    "\n",
    "    out: Dict[WindowSpec, pd.DataFrame] = {}\n",
    "\n",
    "    # (Optional) could precompute different rolling lengths only once.\n",
    "    # We'll cache rolling results by length for efficiency.\n",
    "    rolling_cache: Dict[int, pd.DataFrame] = {}\n",
    "\n",
    "    for spec in windows:\n",
    "        if isinstance(spec, int):\n",
    "            k = spec\n",
    "            if k <= 0:\n",
    "                raise ValueError(f\"Integer window must be positive, got {k}.\")\n",
    "            if k not in rolling_cache:\n",
    "                rolling_cache[k] = log_ret.rolling(window=k, min_periods=k).sum()\n",
    "            summed = rolling_cache[k]\n",
    "            out[spec] = np.expm1(summed)\n",
    "        else:\n",
    "            # Tuple case\n",
    "            if not (isinstance(spec, tuple) and len(spec) == 2):\n",
    "                raise AssertionError(f\"Tuple window must have length 2, got {spec}.\")\n",
    "            a, b = spec\n",
    "            assert isinstance(a, int) and isinstance(b, int), \\\n",
    "                f\"Tuple elements must be ints, got {spec}.\"\n",
    "            assert a > b >= 0, \\\n",
    "                f\"For tuple (a,b) require a > b >= 0; got (a={a}, b={b}).\"\n",
    "\n",
    "            length = a - b\n",
    "            if length not in rolling_cache:\n",
    "                rolling_cache[length] = log_ret.rolling(window=length, min_periods=length).sum()\n",
    "\n",
    "            # Rolling sum over the interval length, then shift forward by b days\n",
    "            # so that the window that *ended* at t-b is aligned to date t.\n",
    "            summed = rolling_cache[length].shift(b)\n",
    "\n",
    "            out[spec] = np.expm1(summed)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91534951",
   "metadata": {},
   "outputs": [],
   "source": [
    "RET_LOOKBACK_WINDOWS = [21, 63, 126, 252, (252, 21), (252, 42), (126, 21)]\n",
    "kday_returns_dict = calc_kday_returns(returns_matrix=returns_matrix, windows=RET_LOOKBACK_WINDOWS)\n",
    "ewm_vol_matrix = ewma_std_strict_window_size(returns_matrix=returns_matrix, span=63, window=253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21abc17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_time_scaled_kday_returns_dict = calc_vol_and_time_scaled_kday_returns(kday_returns_dict, ewm_vol_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f254e",
   "metadata": {},
   "source": [
    "Save kday_returns_dict, vol_time_scaled_kday_returns_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ca77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features/kday_returns_dict.pkl','wb') as f:\n",
    "    pickle.dump(kday_returns_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('features/vol_time_scaled_kday_returns_dict.pkl','wb') as f:\n",
    "    pickle.dump(vol_time_scaled_kday_returns_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1be9f5",
   "metadata": {},
   "source": [
    "Load kday_returns_dict, vol_time_scaled_kday_returns_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('features/kday_returns_dict.pkl','rb') as f:\n",
    "    kday_returns_dict = pickle.load(f)\n",
    "with open('features/vol_time_scaled_kday_returns_dict.pkl','rb') as f:\n",
    "    vol_time_scaled_kday_returns_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652cb723",
   "metadata": {},
   "source": [
    "## MACD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e331a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ew_weights(span=63, window=253, oldest_first=False):\n",
    "    α = 2.0 / (span + 1)                     # pandas/EMA definition\n",
    "    w = (1 - α) ** np.arange(window)         # newest-first\n",
    "    w /= w.sum()                             # renormalise over the truncated window\n",
    "    return w[::-1] if oldest_first else w\n",
    "\n",
    "def ewma_mean_strict_window_size(mat: pd.DataFrame, span: int, window: int = 253) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Finite-window EWMA (length = `window`) that writes NaN unless the\n",
    "    entire look-back window is free of NaNs.\n",
    "\n",
    "    Uses pandasʼ convention  alpha = 2 / (span + 1).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mat : DataFrame T x N\n",
    "    span   : EWMA span S (same interpretation as pd.Series.ewm(span=...)).\n",
    "    window : Truncation length, default 253.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame of the same shape as `prices`.\n",
    "    \"\"\"\n",
    "    if span <= 1:\n",
    "        raise ValueError(\"span must be > 1\")\n",
    "\n",
    "    # -- Convert once to a contiguous NumPy array for speed -------------\n",
    "    x   = mat.to_numpy(dtype=float, copy=False)\n",
    "    T, N = x.shape\n",
    "    out = np.full_like(x, np.nan)\n",
    "\n",
    "    # -- EW coefficients ------------------------------------------------\n",
    "    alpha = 2.0 / (span + 1.0)     # pandas definition\n",
    "    d     = 1.0 - alpha            # decay factor\n",
    "    a     = alpha                  # keep variable names consistent\n",
    "    drop_coef = a * d ** window    # weight leaving the window each step\n",
    "    denom     = 1.0 - d ** window  # normaliser for truncated kernel\n",
    "\n",
    "    num  = np.zeros(N, dtype=float)     # running numerator\n",
    "    run  = np.zeros(N, dtype=np.int32)  # consecutive non-NaN count\n",
    "\n",
    "    for t in range(T):\n",
    "        row  = x[t]\n",
    "        mask_nan = np.isnan(row)\n",
    "\n",
    "        # reset counters where current value is NaN\n",
    "        run[mask_nan] = 0\n",
    "        run[~mask_nan] += 1\n",
    "\n",
    "        # standard EW recursion (treat NaNs as zero contribution here)\n",
    "        num *= d\n",
    "        num += a * np.where(mask_nan, 0.0, row)\n",
    "\n",
    "        # subtract value that just rolled out of the finite window\n",
    "        if t >= window:\n",
    "            old = x[t - window]\n",
    "            num -= drop_coef * np.where(np.isnan(old), 0.0, old)\n",
    "\n",
    "        # output only where we have `window` consecutive valid obs\n",
    "        full = run >= window\n",
    "        if full.any():\n",
    "            out[t, full] = num[full] / denom\n",
    "\n",
    "    return pd.DataFrame(out, index=mat.index, columns=mat.columns)\n",
    "\n",
    "def rolling_std_strict(mat: pd.DataFrame, window: int, valid_frac: float = 0.9) -> pd.DataFrame:\n",
    "    if not (0 <= valid_frac <= 1):\n",
    "        raise ValueError(\"valid_frac must be between 0 and 1\")\n",
    "\n",
    "    min_required = int(np.ceil(window * valid_frac))\n",
    "\n",
    "    std = mat.rolling(window=window, min_periods=min_required).std()\n",
    "    cnt = mat.notna().rolling(window=window, min_periods=window).sum()\n",
    "\n",
    "    # Mask out where count < min_required\n",
    "    std[cnt < min_required] = np.nan\n",
    "    return std\n",
    "\n",
    "\n",
    "def baz_n_to_span(n: int | float) -> float:\n",
    "    \"\"\"\n",
    "    Convert Baz ‘n’ (λ = (n-1)/n, α = 1/n) to the equivalent\n",
    "    pandas EWMA span so that α = 2/(span+1) matches 1/n.\n",
    "    \"\"\"\n",
    "    return 2.0 * n - 1.0\n",
    "\n",
    "def baz_response(z: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Nonlinear response φ(z) = z·exp(–z²/4) / 0.89\n",
    "    \"\"\"\n",
    "    return z * np.exp(-z.pow(2) / 4) / 0.89\n",
    "\n",
    "def calc_macd_scores(\n",
    "    prices: pd.DataFrame,\n",
    "    pairs=((8, 24), (16, 48), (32, 96)),\n",
    "    ew_window: int = 252,\n",
    "    price_vol_window: int = 63,\n",
    "    zscore_window: int = 252,\n",
    "    lags_for_lagged_macd: Dict[str, int] = {\"curr\": 0, \"lag_1m\": 21, \"lag_3m\": 63,\n",
    "                            \"lag_6m\": 126, \"lag_12m\": 252},\n",
    ") -> dict[tuple[int, int], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Baz-style intermediate signals Ẏ̃ (Eq. 6–8).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prices : DataFrame (T × N) of close prices.\n",
    "    pairs  : iterable of Baz ‘n’ numbers (S, L).\n",
    "    ew_window        : truncation length for EWMAs (default 252).\n",
    "    price_vol_window : window for σ_price in Eq. (7) (default 63).\n",
    "    zscore_window    : window for σ_ξ    in Eq. (6) (default 252).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict { (S, L) : DataFrame } – each is Ẏ̃_t^(i)(S,L).\n",
    "    \"\"\"\n",
    "    # --- pre-compute rolling volatilities ---------------------------------\n",
    "    price_std = rolling_std_strict(prices, price_vol_window)\n",
    "\n",
    "    # --- container for results --------------------------------------------\n",
    "    macd_scores = {}\n",
    "\n",
    "    current_y_tildes = []\n",
    "\n",
    "    for S, L in pairs:\n",
    "        # truncated EWMAs\n",
    "        span_s, span_l = baz_n_to_span(S), baz_n_to_span(L)\n",
    "        m_short = ewma_mean_strict_window_size(prices, span=span_s, window=ew_window)\n",
    "        m_long  = ewma_mean_strict_window_size(prices, span=span_l, window=ew_window)\n",
    "        \n",
    "\n",
    "        macd  = m_short - m_long # Eq (8)\n",
    "        xi     = macd / price_std # Eq (7)\n",
    "        \n",
    "        sigma_xi = rolling_std_strict(xi, zscore_window)\n",
    "\n",
    "        y_curr = xi / sigma_xi # Eq (6)\n",
    "        \n",
    "        for label, d in lags_for_lagged_macd.items():\n",
    "            macd_scores[(S, L, label)] = y_curr.shift(d) if d else y_curr\n",
    "        current_y_tildes.append(y_curr)\n",
    "        \n",
    "    # Composite final signal (Eq. 9)\n",
    "    #    Y_t = (1/3) * sum_k φ(Ẏ̃_k)\n",
    "    # responses = [baz_response(df) for df in macd_scores.values()]\n",
    "    composite = sum(baz_response(df) for df in current_y_tildes) / len(pairs)\n",
    "    macd_scores[\"baz_comp\"] = composite\n",
    "\n",
    "    return macd_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaad5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_for_lagged_macd = {\"curr\": 0, \"lag_1m\": 21, \"lag_3m\": 63,\n",
    "                            \"lag_6m\": 126, \"lag_12m\": 252}\n",
    "macd_dict = calc_macd_scores(prices=split_adjusted_close, \n",
    "                             pairs=((8, 24), (16, 48), (32, 96)), \n",
    "                             ew_window=252, price_vol_window=63, \n",
    "                             zscore_window=252,\n",
    "                             lags_for_lagged_macd=lags_for_lagged_macd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b1255",
   "metadata": {},
   "source": [
    "Save and load macd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb6b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open('features/macd_dict.pkl','wb') as f:\n",
    "    pickle.dump(macd_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features/macd_dict.pkl','rb') as f:\n",
    "    macd_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c29875",
   "metadata": {},
   "source": [
    "## Rolling betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b420eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_vix_df = pd.read_csv('features/spy_vix_changes.csv')\n",
    "spy_vix_df.index = pd.to_datetime(spy_vix_df['date'])\n",
    "spy_ret_df = spy_vix_df[['SPY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c84ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def _ewm_betas_strict_numba(X: np.ndarray,\n",
    "                            m: np.ndarray,\n",
    "                            span: int,\n",
    "                            window: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute EW-weighted rolling betas of each column in X vs market m,\n",
    "    using exactly the most recent `window` points with EWM decay `span`.\n",
    "    Any NaN in the window (either series) -> beta is NaN for that date.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : float64 array, shape (T, N)\n",
    "        Asset returns (aligned to market).\n",
    "    m : float64 array, shape (T,)\n",
    "        Market (SPY) returns (aligned to X).\n",
    "    span : int\n",
    "        EW span (alpha = 2/(span+1)).\n",
    "    window : int\n",
    "        Exact window length.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    betas : float64 array, shape (T, N)\n",
    "        Rolling betas; first `window-1` rows are NaN.\n",
    "    \"\"\"\n",
    "    T, N = X.shape\n",
    "\n",
    "    alpha = 2.0 / (span + 1.0)\n",
    "    gamma = 1.0 - alpha               # decay\n",
    "    g_pow_w = gamma ** window         # gamma^window\n",
    "    # Sum of raw weights over the finite window (constant):\n",
    "    S1 = (1.0 - g_pow_w) / (1.0 - gamma)\n",
    "\n",
    "    betas = np.empty((T, N), dtype=np.float64)\n",
    "    betas[:] = np.nan\n",
    "\n",
    "    # Running (unnormalized) weighted sums; newest obs weight = 1\n",
    "    Sx = np.zeros(N, dtype=np.float64)     # sum gamma^i * x\n",
    "    Sxm = np.zeros(N, dtype=np.float64)    # sum gamma^i * x*m\n",
    "    Sm = 0.0                               # sum gamma^i * m\n",
    "    Smm = 0.0                              # sum gamma^i * m^2\n",
    "\n",
    "    # Track how many valid (non-NaN pair) points are in the last window\n",
    "    counts = np.zeros(N, dtype=np.int32)\n",
    "    valid_buf = np.zeros((window, N), dtype=np.uint8)  # ring buffer of 0/1\n",
    "\n",
    "    for t in range(T):\n",
    "        m_new = m[t]\n",
    "        m_new_val = 0.0 if np.isnan(m_new) else m_new\n",
    "\n",
    "        m_old_val = 0.0\n",
    "        if t >= window:\n",
    "            m_old = m[t - window]\n",
    "            m_old_val = 0.0 if np.isnan(m_old) else m_old\n",
    "\n",
    "        # Update market sums (shared across all assets)\n",
    "        Sm = gamma * Sm + m_new_val - g_pow_w * m_old_val\n",
    "        Smm = gamma * Smm + m_new_val * m_new_val - g_pow_w * (m_old_val * m_old_val)\n",
    "\n",
    "        # Denominator (unnormalized weighted var of market after demeaning)\n",
    "        denom_unscaled = Smm - (Sm * Sm) / S1\n",
    "\n",
    "        pos = t % window  # ring-buffer position for this step\n",
    "\n",
    "        # Per-asset updates\n",
    "        for j in prange(N):\n",
    "            x_new = X[t, j]\n",
    "            x_new_val = 0.0 if np.isnan(x_new) else x_new\n",
    "\n",
    "            x_old_val = 0.0\n",
    "            if t >= window:\n",
    "                x_old = X[t - window, j]\n",
    "                x_old_val = 0.0 if np.isnan(x_old) else x_old\n",
    "\n",
    "            # Update asset sums\n",
    "            Sx[j] = gamma * Sx[j] + x_new_val - g_pow_w * x_old_val\n",
    "            Sxm[j] = gamma * Sxm[j] + (x_new_val * m_new_val) - g_pow_w * (x_old_val * m_old_val)\n",
    "\n",
    "            # Maintain strict-window validity count (both series must be finite)\n",
    "            new_valid = 1 if (not np.isnan(x_new) and not np.isnan(m_new)) else 0\n",
    "            old_valid = 0\n",
    "            if t >= window:\n",
    "                old_valid = valid_buf[pos, j]\n",
    "            valid_buf[pos, j] = new_valid\n",
    "            counts[j] += (new_valid - old_valid)\n",
    "\n",
    "            # Emit beta only when the last `window` pairs are all valid\n",
    "            if t >= window - 1 and counts[j] == window and denom_unscaled > 1e-14:\n",
    "                # Numerator is (cov * S1); denominator is (var_m * S1); S1 cancels in the ratio\n",
    "                num_unscaled = Sxm[j] - (Sx[j] * Sm) / S1\n",
    "                betas[t, j] = num_unscaled / denom_unscaled\n",
    "            else:\n",
    "                betas[t, j] = np.nan\n",
    "\n",
    "    return betas\n",
    "\n",
    "\n",
    "def rolling_beta_ewm_strict(\n",
    "    returns_matrix: pd.DataFrame,\n",
    "    spy_returns: pd.Series | pd.DataFrame,\n",
    "    span: int = 63,\n",
    "    window: int = 253,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rolling CAPM beta (with intercept) vs SPY using a strict, finite EW window.\n",
    "\n",
    "    - Uses EW weights with `span` (alpha = 2/(span+1)), applied over exactly\n",
    "      the last `window` days (finite horizon).\n",
    "    - If *any* missing value appears in the `window` for either series,\n",
    "      the beta for that date is NaN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns_matrix : DataFrame (dates × assets)\n",
    "        Daily asset returns.\n",
    "    spy_returns : Series or 1-col DataFrame\n",
    "        Daily SPY returns.\n",
    "    span : int\n",
    "        EWM span.\n",
    "    window : int\n",
    "        Exact window length.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Betas with the same index/columns as `returns_matrix`.\n",
    "    \"\"\"\n",
    "    if isinstance(spy_returns, pd.DataFrame):\n",
    "        if spy_returns.shape[1] != 1:\n",
    "            raise ValueError(\"spy_returns must be a Series or a 1-column DataFrame.\")\n",
    "        spy_returns = spy_returns.iloc[:, 0]\n",
    "\n",
    "    # Align on common dates (strictly)\n",
    "    idx = returns_matrix.index.intersection(spy_returns.index)\n",
    "    idx = idx.sort_values()\n",
    "\n",
    "    X = returns_matrix.loc[idx].to_numpy(dtype=np.float64, copy=True)\n",
    "    m = spy_returns.loc[idx].to_numpy(dtype=np.float64, copy=True)\n",
    "\n",
    "    betas = _ewm_betas_strict_numba(X, m, span, window)\n",
    "    return pd.DataFrame(betas, index=idx, columns=returns_matrix.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f381df",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_df = rolling_beta_ewm_strict(returns_matrix, spy_ret_df[\"SPY\"], span=63, window=253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19536beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_df.columns = beta_df.columns.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63291930",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_df.to_hdf('features/beta_df.h5', key='beta_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec53734c",
   "metadata": {},
   "source": [
    "## Assembling raw return, vol-scaled return and MACD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e628bf85",
   "metadata": {},
   "source": [
    "### Filter by\n",
    "#### a) All features on the day of trading are present (we need close to 2 years of burn-in period)\n",
    "#### b) Get the top 1000 by mkt cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3019196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mask_for_tradeable_stocks(\n",
    "    feature_dicts: Sequence[Dict[Any, pd.DataFrame]],\n",
    "    existing_mask_for_tradeable_stocks: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a boolean DataFrame (`True` = the stock-date survives **all**\n",
    "    feature-NaN checks + the external tradability filter).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_dicts : sequence of the dicts that hold your feature matrices\n",
    "                    (e.g. [kday_returns_dict, vol_scaled_dict, macd_dict]).\n",
    "    existing_mask : initial boolean mask of tradeable stocks, same shape/idx.\n",
    "    \"\"\"\n",
    "    mask = existing_mask_for_tradeable_stocks.copy()\n",
    "    for d in feature_dicts:\n",
    "        for mat in d.values():\n",
    "            mask &= ~(mat.isna())\n",
    "    return mask\n",
    "\n",
    "def build_stocks_to_trade_mask_using_mcap(\n",
    "    trade_mask: pd.DataFrame,\n",
    "    mcap_wide: pd.DataFrame,\n",
    "    num_stocks_to_trade: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds the market-cap ranking filter on top of `trade_mask`.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        mcap_wide.where(trade_mask)\n",
    "                 .rank(axis=1, method=\"first\", ascending=False)\n",
    "                 <= num_stocks_to_trade\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_for_tradeable_stocks = update_mask_for_tradeable_stocks(\n",
    "    [kday_returns_dict, avg_turnover_dict],\n",
    "    # [kday_returns_dict, vol_time_scaled_kday_returns_dict, macd_dict],\n",
    "    valid253\n",
    ")\n",
    "mask_for_stocks_to_trade = build_stocks_to_trade_mask_using_mcap(\n",
    "    mask_for_tradeable_stocks,\n",
    "    mcap_wide_aligned,\n",
    "    NUM_STOCKS_TO_TRADE\n",
    ")   \n",
    "# del mask_for_tradeable_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8677da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_for_tradeable_stocks.to_hdf(\"features/mask_for_tradeable_stocks.h5\",\n",
    "                                 key=\"mask_for_tradeable_stocks\",\n",
    "                                 format=\"fixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c69a20c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feat_mat_for_ltr(\n",
    "    kday_returns_dict: Dict[int, pd.DataFrame],\n",
    "    vol_time_scaled_kday_returns_dict: Dict[int, pd.DataFrame],\n",
    "    macd_dict: Dict[Tuple[int, int], pd.DataFrame],\n",
    "    avg_turnover_dict: Dict[str, pd.DataFrame],\n",
    "    mask_for_stocks_to_trade: pd.DataFrame,\n",
    "    drop_rows_all_nan: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop-in replacement. Produces the same result as the original implementation:\n",
    "      - same rows/cols/order\n",
    "      - same dtypes/NaNs\n",
    "      - same sorting and column names\n",
    "    \"\"\"\n",
    "\n",
    "    def _series(name: str, mat: pd.DataFrame) -> pd.Series:\n",
    "        # exactly the same expression as original\n",
    "        return mat.where(mask_for_stocks_to_trade).stack().rename(name)\n",
    "\n",
    "    X = None\n",
    "    col_order = []\n",
    "\n",
    "    # 1) kday_returns_dict\n",
    "    for k, mat in kday_returns_dict.items():\n",
    "        name = f\"{k}_day_raw_ret\"\n",
    "        s = _series(name, mat)\n",
    "        col_order.append(name)\n",
    "        if X is None:\n",
    "            X = s.to_frame()\n",
    "        else:\n",
    "            X[name] = s  # align on index\n",
    "\n",
    "    # 2) vol_time_scaled_kday_returns_dict\n",
    "    for k, mat in vol_time_scaled_kday_returns_dict.items():\n",
    "        name = f\"{k}_day_vol_time_scaled_ret\"\n",
    "        s = _series(name, mat)\n",
    "        col_order.append(name)\n",
    "        X[name] = s\n",
    "\n",
    "    # 3) macd_dict\n",
    "    for k, mat in macd_dict.items():\n",
    "        col_name = \"_\".join(map(str, k)) if isinstance(k, tuple) else k\n",
    "        s = _series(col_name, mat)\n",
    "        col_order.append(col_name)\n",
    "        X[col_name] = s\n",
    "\n",
    "    # 4) avg_turnover_dict\n",
    "    for k, mat in avg_turnover_dict.items():\n",
    "        s = _series(k, mat)\n",
    "        col_order.append(k)\n",
    "        X[k] = s\n",
    "\n",
    "    # Ensure identical column order to the original concat\n",
    "    X = X[col_order]\n",
    "\n",
    "    final_df = (\n",
    "        X.reset_index()                               # index → cols\n",
    "         .rename(columns={'level_0': 'date', 'level_1': 'permno'})\n",
    "         .sort_values(['date', 'permno'])\n",
    "    )\n",
    "\n",
    "    if drop_rows_all_nan:\n",
    "        final_df = final_df.dropna(how='all', subset=final_df.columns[2:])\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a1bf8dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward_deciles(\n",
    "    *,\n",
    "    returns_matrix: pd.DataFrame,\n",
    "    mask_for_stocks_to_trade: pd.DataFrame,\n",
    "    feature_index_df: pd.DataFrame,          # feat_mat[[\"date\",\"permno\"]]\n",
    "    target_horizon: int = 21,\n",
    "    num_deciles: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # --- 1) trailing 21-day cumulative return ------------------------\n",
    "    trailing_ret = (\n",
    "        (1 + returns_matrix)\n",
    "        .rolling(target_horizon, min_periods=target_horizon)\n",
    "        .apply(np.prod, raw=True)\n",
    "        .sub(1)\n",
    "    )\n",
    "\n",
    "    # --- 2) shift back by 21 to make it \"forward\" --------------------\n",
    "    fwd_return = trailing_ret.shift(-target_horizon)\n",
    "    \n",
    "    # --- 3) rank into (approximately) equal-sized *per-date* deciles --\n",
    "    # Mask out untradeable stocks first\n",
    "    fwd_masked = fwd_return.where(mask_for_stocks_to_trade)\n",
    "\n",
    "    def _row_deciles(row: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Assign deciles 1..num_deciles within a single date (row),\n",
    "        using qcut for (near) equal-sized buckets. Keeps NaNs for masked entries.\n",
    "        \"\"\"\n",
    "        valid = row.dropna()\n",
    "        n = len(valid)\n",
    "        if n == 0:\n",
    "            return pd.Series(index=row.index, dtype=\"Int8\")\n",
    "\n",
    "        # If fewer valid observations than desired deciles, fall back to rank-based\n",
    "        # buckets so we still return integers starting at 1 (some top deciles unused).\n",
    "        if n < num_deciles:\n",
    "            # dense rank 1..n, then map linearly into 1..num_deciles using ceil\n",
    "            r = valid.rank(method=\"first\")\n",
    "            dec = np.ceil(r / n * num_deciles).astype(int)\n",
    "            return dec.reindex(row.index).astype(\"Int8\")\n",
    "\n",
    "        # Use qcut; duplicates='drop' handles many ties at boundaries (may yield < num_deciles bins)\n",
    "        bins = pd.qcut(\n",
    "            valid,\n",
    "            q=num_deciles,\n",
    "            labels=range(1, num_deciles + 1),\n",
    "            duplicates=\"drop\",\n",
    "        )\n",
    "\n",
    "        # If we lost some bins due to ties (e.g., only 8 unique bins), re-label consecutively\n",
    "        # so that result still starts at 1 and is compact.\n",
    "        u = pd.unique(bins)\n",
    "        if len(u) < num_deciles:\n",
    "            remap = {old: i + 1 for i, old in enumerate(sorted(u))}\n",
    "            bins = bins.map(remap)\n",
    "\n",
    "        return bins.reindex(row.index).astype(\"Int8\")\n",
    "\n",
    "    # Apply per row (date)\n",
    "    decile_df = fwd_masked.apply(_row_deciles, axis=1)\n",
    "\n",
    "    # --- 4) same reshaping / joining as before -----------------------\n",
    "    y_ret_decile = (\n",
    "        decile_df.stack(dropna=False)\n",
    "                 .rename(\"decile\")\n",
    "                 .to_frame()\n",
    "                 .join(\n",
    "                     fwd_return.stack(dropna=False).rename(\"fwd_ret\")\n",
    "                 )\n",
    "                 .reset_index()\n",
    "                 .rename(columns={\"level_0\": \"date\",\n",
    "                                  \"level_1\": \"permno\"})\n",
    "                 .dropna(subset=[\"decile\"])\n",
    "                 .merge(feature_index_df, on=[\"date\", \"permno\"], how=\"inner\")\n",
    "                 .sort_values([\"date\", \"permno\"])\n",
    "                 .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    y_ret_decile[\"decile\"] = y_ret_decile[\"decile\"].astype(\"UInt8\")\n",
    "    y_ret_decile[\"fwd_ret\"] = y_ret_decile[\"fwd_ret\"].astype(\"float32\")\n",
    "\n",
    "    return y_ret_decile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5dcbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ltr_xy(\n",
    "    *,\n",
    "    kday_returns_dict,                       # same signatures as before\n",
    "    vol_time_scaled_kday_returns_dict,\n",
    "    macd_dict,\n",
    "    avg_turnover_dict,\n",
    "    mask_for_stocks_to_trade,\n",
    "    returns_matrix,\n",
    "    target_horizon: int = 21,\n",
    "    num_deciles: int = 10,\n",
    "    drop_rows_all_nan: bool = True,\n",
    "):\n",
    "    X = generate_feat_mat_for_ltr(\n",
    "        kday_returns_dict=kday_returns_dict,\n",
    "        vol_time_scaled_kday_returns_dict=vol_time_scaled_kday_returns_dict,\n",
    "        macd_dict=macd_dict,\n",
    "        avg_turnover_dict=avg_turnover_dict,\n",
    "        mask_for_stocks_to_trade=mask_for_stocks_to_trade,\n",
    "        drop_rows_all_nan=drop_rows_all_nan,\n",
    "    )\n",
    "    \n",
    "\n",
    "    y = compute_forward_deciles(\n",
    "        returns_matrix=returns_matrix,\n",
    "        mask_for_stocks_to_trade=mask_for_stocks_to_trade,\n",
    "        feature_index_df=X[['date', 'permno']],\n",
    "        target_horizon=target_horizon,\n",
    "        num_deciles=num_deciles\n",
    "    )\n",
    "    \n",
    "    Xy_df = (\n",
    "        X.merge(y, on=['date', 'permno'], how='inner')\n",
    "        .sort_values(['date', 'permno'])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    return Xy_df\n",
    "\n",
    "ltr_df = build_ltr_xy(\n",
    "    kday_returns_dict=kday_returns_dict,\n",
    "    vol_time_scaled_kday_returns_dict=vol_time_scaled_kday_returns_dict,\n",
    "    macd_dict=macd_dict,\n",
    "    avg_turnover_dict=avg_turnover_dict,\n",
    "    mask_for_stocks_to_trade=mask_for_stocks_to_trade,\n",
    "    returns_matrix=returns_matrix,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f821dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_df.to_csv('features/ltr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ebd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_df.to_hdf(\n",
    "    'features/ltr.h5',\n",
    "    key='ltr',\n",
    "    mode='w',\n",
    "    complib='zlib',\n",
    "    complevel=9\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf7163",
   "metadata": {},
   "source": [
    "# Heavy batching (use this to generate ltr_df if the above cells cause memory issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _feature_name_list(\n",
    "    kday_returns_dict,\n",
    "    vol_time_scaled_kday_returns_dict,\n",
    "    macd_dict,\n",
    "    avg_turnover_dict\n",
    "):\n",
    "    names = []\n",
    "    for k in kday_returns_dict:                     # 1\n",
    "        names.append(f\"{k}_day_raw_ret\")\n",
    "    for k in vol_time_scaled_kday_returns_dict:     # 2\n",
    "        names.append(f\"{k}_day_vol_time_scaled_ret\")\n",
    "    for k in macd_dict:                             # 3\n",
    "        names.append(\"_\".join(map(str, k)) if isinstance(k, tuple) else k)\n",
    "    for k in avg_turnover_dict:                     # 4\n",
    "        names.append(k)\n",
    "    return names\n",
    "\n",
    "\n",
    "def generate_feat_mat_for_ltr(\n",
    "    kday_returns_dict: Dict[int, pd.DataFrame],\n",
    "    vol_time_scaled_kday_returns_dict: Dict[int, pd.DataFrame],\n",
    "    macd_dict: Dict[Tuple[int, int], pd.DataFrame],\n",
    "    avg_turnover_dict: Dict[str, pd.DataFrame],\n",
    "    mask_for_stocks_to_trade: pd.DataFrame,\n",
    "    drop_rows_all_nan: bool = True,\n",
    "    *,\n",
    "    hdf_path: str = \"X_features.h5\",     # where to store intermediate/final\n",
    "    key: str = \"X\",                      # HDF5 group/key\n",
    "    date_batch_size: int = 250,          # adjust to your RAM\n",
    "    return_in_memory: bool = False       # False → just write to HDF\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Writes the full wide feature matrix to an HDF5 table in row batches.\n",
    "    If return_in_memory is True, it also loads & returns the whole DataFrame\n",
    "    (identical to the original function's output).\n",
    "    \"\"\"\n",
    "    # Precompute final column order (matches original concat order)\n",
    "    col_order = _feature_name_list(\n",
    "        kday_returns_dict,\n",
    "        vol_time_scaled_kday_returns_dict,\n",
    "        macd_dict,\n",
    "        avg_turnover_dict\n",
    "    )\n",
    "\n",
    "    # Ensure file is clean\n",
    "    try:\n",
    "        with pd.HDFStore(hdf_path, mode=\"w\") as _:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    dates = mask_for_stocks_to_trade.index\n",
    "    n_dates = len(dates)\n",
    "\n",
    "    with pd.HDFStore(hdf_path, mode=\"a\") as store:\n",
    "        for start in range(0, n_dates, date_batch_size):\n",
    "            end = min(start + date_batch_size, n_dates)\n",
    "            d_slice = dates[start:end]\n",
    "\n",
    "            mask_chunk = mask_for_stocks_to_trade.loc[d_slice]\n",
    "\n",
    "            series_list = []\n",
    "\n",
    "            # 1) kday_returns_dict\n",
    "            for k, mat in kday_returns_dict.items():\n",
    "                name = f\"{k}_day_raw_ret\"\n",
    "                s = mat.loc[d_slice].where(mask_chunk).stack().rename(name)\n",
    "                series_list.append(s)\n",
    "\n",
    "            # 2) vol_time_scaled_kday_returns_dict\n",
    "            for k, mat in vol_time_scaled_kday_returns_dict.items():\n",
    "                name = f\"{k}_day_vol_time_scaled_ret\"\n",
    "                s = mat.loc[d_slice].where(mask_chunk).stack().rename(name)\n",
    "                series_list.append(s)\n",
    "\n",
    "            # 3) macd_dict\n",
    "            for k, mat in macd_dict.items():\n",
    "                col_name = \"_\".join(map(str, k)) if isinstance(k, tuple) else k\n",
    "                s = mat.loc[d_slice].where(mask_chunk).stack().rename(col_name)\n",
    "                series_list.append(s)\n",
    "\n",
    "            # 4) avg_turnover_dict\n",
    "            for k, mat in avg_turnover_dict.items():\n",
    "                s = mat.loc[d_slice].where(mask_chunk).stack().rename(k)\n",
    "                series_list.append(s)\n",
    "\n",
    "            chunk_df = pd.concat(series_list, axis=1)\n",
    "            chunk_df = (\n",
    "                chunk_df.reset_index()\n",
    "                        .rename(columns={\"level_0\": \"date\", \"level_1\": \"permno\"})\n",
    "                        .sort_values([\"date\", \"permno\"])\n",
    "            )\n",
    "\n",
    "            if drop_rows_all_nan:\n",
    "                chunk_df = chunk_df.dropna(how=\"all\", subset=col_order)\n",
    "\n",
    "            # reorder cols precisely once\n",
    "            chunk_df = chunk_df[[\"date\", \"permno\"] + col_order]\n",
    "\n",
    "            # Append rows to HDF5 table (same schema for each chunk)\n",
    "            store.append(key, chunk_df, format=\"table\", data_columns=[\"date\", \"permno\"])\n",
    "\n",
    "            # free memory\n",
    "            del series_list, chunk_df, mask_chunk\n",
    "            gc.collect()\n",
    "\n",
    "    if not return_in_memory:\n",
    "        return None\n",
    "\n",
    "    # Load the full thing back (identical to original output)\n",
    "    with pd.HDFStore(hdf_path, mode=\"r\") as store:\n",
    "        full_df = store.select(key)\n",
    "    return full_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "de5e76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward_deciles_batched(\n",
    "    *,\n",
    "    returns_matrix: pd.DataFrame,\n",
    "    mask_for_stocks_to_trade: pd.DataFrame,\n",
    "    feature_index_df: pd.DataFrame,     # X[['date','permno']]\n",
    "    target_horizon: int = 21,\n",
    "    num_deciles: int = 10,\n",
    "    hdf_path: str = \"y_targets.h5\",\n",
    "    key: str = \"y\",\n",
    "    date_batch_size: int = 250,\n",
    "    return_in_memory: bool = False\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "\n",
    "    # 1) trailing & forward returns (full, needed to compute deciles consistently)\n",
    "    trailing_ret = (\n",
    "        (1 + returns_matrix)\n",
    "        .rolling(target_horizon, min_periods=target_horizon)\n",
    "        .apply(np.prod, raw=True)\n",
    "        .sub(1)\n",
    "    )\n",
    "    fwd_return = trailing_ret.shift(-target_horizon)\n",
    "    fwd_masked = fwd_return.where(mask_for_stocks_to_trade)\n",
    "\n",
    "    def _row_deciles(row: pd.Series) -> pd.Series:\n",
    "        valid = row.dropna()\n",
    "        n = len(valid)\n",
    "        if n == 0:\n",
    "            return pd.Series(index=row.index, dtype=\"Int8\")\n",
    "        if n < num_deciles:\n",
    "            r = valid.rank(method=\"first\")\n",
    "            dec = np.ceil(r / n * num_deciles).astype(int)\n",
    "            return dec.reindex(row.index).astype(\"Int8\")\n",
    "        bins = pd.qcut(valid, q=num_deciles,\n",
    "                       labels=range(1, num_deciles + 1),\n",
    "                       duplicates=\"drop\")\n",
    "        u = pd.unique(bins)\n",
    "        if len(u) < num_deciles:\n",
    "            remap = {old: i + 1 for i, old in enumerate(sorted(u))}\n",
    "            bins = bins.map(remap)\n",
    "        return bins.reindex(row.index).astype(\"Int8\")\n",
    "\n",
    "    dates = fwd_masked.index\n",
    "    with pd.HDFStore(hdf_path, mode=\"w\") as store:\n",
    "        for start in range(0, len(dates), date_batch_size):\n",
    "            end = min(start + date_batch_size, len(dates))\n",
    "            d_slice = dates[start:end]\n",
    "\n",
    "            decile_chunk = fwd_masked.loc[d_slice].apply(_row_deciles, axis=1)\n",
    "            # stack both\n",
    "            y_chunk = (\n",
    "                decile_chunk.stack(dropna=False).rename(\"decile\").to_frame()\n",
    "                .join(fwd_return.loc[d_slice].stack(dropna=False).rename(\"fwd_ret\"))\n",
    "                .reset_index()\n",
    "                .rename(columns={\"level_0\": \"date\", \"level_1\": \"permno\"})\n",
    "                .dropna(subset=[\"decile\"])\n",
    "            )\n",
    "\n",
    "            # merge with feature_index_df (only rows present in the features)\n",
    "            idx_chunk = feature_index_df[\n",
    "                (feature_index_df[\"date\"] >= d_slice[0]) &\n",
    "                (feature_index_df[\"date\"] <= d_slice[-1])\n",
    "            ]\n",
    "            y_chunk = (\n",
    "                y_chunk.merge(idx_chunk, on=[\"date\", \"permno\"], how=\"inner\")\n",
    "                       .sort_values([\"date\", \"permno\"])\n",
    "                       .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            # cast to compact types\n",
    "            y_chunk[\"decile\"] = y_chunk[\"decile\"].astype(\"UInt8\")\n",
    "            y_chunk[\"fwd_ret\"] = y_chunk[\"fwd_ret\"].astype(\"float32\")\n",
    "\n",
    "            store.append(key, y_chunk, format=\"table\", data_columns=[\"date\", \"permno\"])\n",
    "\n",
    "            del decile_chunk, y_chunk, idx_chunk\n",
    "            gc.collect()\n",
    "\n",
    "    if not return_in_memory:\n",
    "        return None\n",
    "    with pd.HDFStore(hdf_path, mode=\"r\") as store:\n",
    "        return store.select(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7cb4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _row_deciles(row: pd.Series, num_deciles: int) -> pd.Series:\n",
    "    \"\"\"Assign per-row deciles with NA handling.\"\"\"\n",
    "    valid = row.dropna()\n",
    "    n = len(valid)\n",
    "    if n == 0:\n",
    "        return pd.Series(index=row.index, dtype=\"Int8\")\n",
    "    if n < num_deciles:\n",
    "        r = valid.rank(method=\"first\")\n",
    "        dec = np.ceil(r / n * num_deciles).astype(int)\n",
    "        return dec.reindex(row.index).astype(\"Int8\")\n",
    "    bins = pd.qcut(\n",
    "        valid,\n",
    "        q=num_deciles,\n",
    "        labels=range(1, num_deciles + 1),\n",
    "        duplicates=\"drop\",\n",
    "    )\n",
    "    u = pd.unique(bins)\n",
    "    if len(u) < num_deciles:\n",
    "        remap = {old: i + 1 for i, old in enumerate(sorted(u))}\n",
    "        bins = bins.map(remap)\n",
    "    return bins.reindex(row.index).astype(\"Int8\")\n",
    "\n",
    "\n",
    "def build_ltr_xy_batched(\n",
    "    *,\n",
    "    kday_returns_dict: Dict[int, pd.DataFrame],\n",
    "    vol_time_scaled_kday_returns_dict: Dict[int, pd.DataFrame],\n",
    "    macd_dict: Dict[Tuple[int, int], pd.DataFrame],\n",
    "    avg_turnover_dict: Dict[str, pd.DataFrame],\n",
    "    mask_for_stocks_to_trade: pd.DataFrame,\n",
    "    returns_matrix: pd.DataFrame,\n",
    "    target_horizon: int = 21,\n",
    "    num_deciles: int = 10,\n",
    "    date_batch_size: int = 250,\n",
    "    hdf_path: str = \"ltr.h5\",\n",
    "    key: str = \"ltr\",\n",
    "    return_in_memory: bool = False,\n",
    "    use_existing_X: bool = False,\n",
    "    use_existing_Y: bool = False,\n",
    "    X_path: str = \"X_features.h5\",\n",
    "    X_key: str = \"X\",\n",
    "    Y_path: str = \"y_targets.h5\",\n",
    "    Y_key: str = \"y\",\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Build or reuse X and Y, then merge to LTR in date batches.\"\"\"\n",
    "    # Dates to iterate over come from the mask\n",
    "    all_dates = mask_for_stocks_to_trade.index.to_numpy()\n",
    "\n",
    "    # X\n",
    "    if not use_existing_X:\n",
    "        generate_feat_mat_for_ltr(\n",
    "            kday_returns_dict=kday_returns_dict,\n",
    "            vol_time_scaled_kday_returns_dict=vol_time_scaled_kday_returns_dict,\n",
    "            macd_dict=macd_dict,\n",
    "            avg_turnover_dict=avg_turnover_dict,\n",
    "            mask_for_stocks_to_trade=mask_for_stocks_to_trade,\n",
    "            drop_rows_all_nan=True,\n",
    "            hdf_path=X_path,\n",
    "            key=X_key,\n",
    "            date_batch_size=date_batch_size,\n",
    "            return_in_memory=False,\n",
    "        )\n",
    "\n",
    "    # Y\n",
    "    if not use_existing_Y:\n",
    "        trailing_ret = (\n",
    "            (1 + returns_matrix)\n",
    "            .rolling(target_horizon, min_periods=target_horizon)\n",
    "            .apply(np.prod, raw=True)\n",
    "            .sub(1)\n",
    "        )\n",
    "        fwd_return = trailing_ret.shift(-target_horizon)\n",
    "        fwd_masked = fwd_return.where(mask_for_stocks_to_trade)\n",
    "\n",
    "        with pd.HDFStore(Y_path, \"w\") as Y_store:\n",
    "            for start in range(0, len(all_dates), date_batch_size):\n",
    "                end = min(start + date_batch_size, len(all_dates))\n",
    "                d0, d1 = all_dates[start], all_dates[end - 1]\n",
    "\n",
    "                decile_chunk = fwd_masked.loc[d0:d1].apply(\n",
    "                    _row_deciles, axis=1, num_deciles=num_deciles\n",
    "                )\n",
    "\n",
    "                y_chunk = (\n",
    "                    decile_chunk.stack(dropna=False)\n",
    "                    .rename(\"decile\")\n",
    "                    .to_frame()\n",
    "                    .join(\n",
    "                        fwd_return.loc[d0:d1]\n",
    "                        .stack(dropna=False)\n",
    "                        .rename(\"fwd_ret\")\n",
    "                    )\n",
    "                    .reset_index()\n",
    "                    .rename(columns={\"level_0\": \"date\", \"level_1\": \"permno\"})\n",
    "                    .dropna(subset=[\"decile\"])\n",
    "                )\n",
    "\n",
    "                # compact dtypes\n",
    "                y_chunk[\"decile\"] = y_chunk[\"decile\"].astype(\"int8\")\n",
    "                y_chunk[\"fwd_ret\"] = y_chunk[\"fwd_ret\"].astype(\"float32\")\n",
    "\n",
    "                Y_store.append(\n",
    "                    Y_key, y_chunk, format=\"table\", data_columns=[\"date\", \"permno\"]\n",
    "                )\n",
    "\n",
    "                del decile_chunk, y_chunk\n",
    "                gc.collect()\n",
    "\n",
    "        del trailing_ret, fwd_return, fwd_masked\n",
    "        gc.collect()\n",
    "\n",
    "    # Merge X & Y -> LTR\n",
    "    with pd.HDFStore(hdf_path, \"w\") as out_store, \\\n",
    "         pd.HDFStore(X_path, \"r\") as X_store, \\\n",
    "         pd.HDFStore(Y_path, \"r\") as Y_store:\n",
    "\n",
    "        for start in range(0, len(all_dates), date_batch_size):\n",
    "            end = min(start + date_batch_size, len(all_dates))\n",
    "            d0, d1 = all_dates[start], all_dates[end - 1]\n",
    "\n",
    "            # Interpolate datetimes into the where clause as ISO strings\n",
    "            where_dates = [\n",
    "                f\"date >= '{pd.Timestamp(d0)}'\",\n",
    "                f\"date <= '{pd.Timestamp(d1)}'\",\n",
    "            ]\n",
    "\n",
    "            X_chunk = X_store.select(X_key, where=where_dates)\n",
    "            if X_chunk.empty:\n",
    "                continue\n",
    "\n",
    "            Y_chunk = Y_store.select(Y_key, where=where_dates)\n",
    "\n",
    "            chunk = (\n",
    "                X_chunk.merge(Y_chunk, on=[\"date\", \"permno\"], how=\"inner\")\n",
    "                .sort_values([\"date\", \"permno\"])\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "            out_store.append(\n",
    "                key, chunk, format=\"table\", data_columns=[\"date\", \"permno\"]\n",
    "            )\n",
    "\n",
    "            del X_chunk, Y_chunk, chunk\n",
    "            gc.collect()\n",
    "\n",
    "    if not return_in_memory:\n",
    "        return None\n",
    "\n",
    "    with pd.HDFStore(hdf_path, \"r\") as store:\n",
    "        return store.select(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86fe0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build directly to disk, no huge objects kept:\n",
    "ltr_df = build_ltr_xy_batched(\n",
    "    kday_returns_dict=kday_returns_dict,\n",
    "    vol_time_scaled_kday_returns_dict=vol_time_scaled_kday_returns_dict,\n",
    "    macd_dict=macd_dict,\n",
    "    avg_turnover_dict=avg_turnover_dict,\n",
    "    mask_for_stocks_to_trade=mask_for_stocks_to_trade,\n",
    "    returns_matrix=returns_matrix,\n",
    "    date_batch_size=200,\n",
    "    return_in_memory=False,  # keep False if RAM is tight\n",
    "    hdf_path=\"ltr.h5\",\n",
    "    key=\"ltr\",\n",
    "    use_existing_X=True\n",
    ")\n",
    "\n",
    "ltr_df.to_hdf(\n",
    "    'features/ltr.h5',\n",
    "    key='ltr',\n",
    "    mode='w',\n",
    "    complib='zlib',\n",
    "    complevel=9\n",
    ")\n",
    "\n",
    "# with pd.HDFStore(\"ltr.h5\", \"r\") as store:\n",
    "#     ltr_df = store.select(\"ltr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492653d5",
   "metadata": {},
   "source": [
    "# Generate Top-1000 Stock Mask from ltr_df (need for BOCD feat generation code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbb534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_top1000_mask_from_ltr(\n",
    "    ltr_long_df: pd.DataFrame,\n",
    "    returns_wide: pd.DataFrame | None = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a boolean mask indicating whether (date, permno) appears in ltr_df.\n",
    "\n",
    "    If `returns_wide` is provided, the mask is reindexed to precisely match\n",
    "    its index/columns and any missing (date, permno) pairs are filled as False.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ltr_long_df : DataFrame with at least ['date', 'permno'] columns.\n",
    "    returns_wide : Optional wide returns matrix to align the mask (index=date, columns=permno).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame[bool] indexed by date with PERMNO columns.\n",
    "    \"\"\"\n",
    "    # Ensure 'date' is datetime; avoid re-reading anything from disk.\n",
    "    df = ltr_long_df[['date', 'permno']].copy()\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['date']):\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "    # Cross-tabulate presence: True iff that (date, permno) appears in ltr_df\n",
    "    mask = pd.crosstab(df['date'], df['permno']).gt(0)\n",
    "\n",
    "    if returns_wide is not None:\n",
    "        # Align to returns matrix shape to keep downstream matrices perfectly conformable\n",
    "        mask = mask.reindex(index=returns_wide.index, columns=returns_wide.columns).fillna(False)\n",
    "\n",
    "    # Enforce boolean dtype consistently\n",
    "    return mask.astype('bool')\n",
    "\n",
    "\n",
    "# Build mask from the in-memory ltr_df (and align to returns_matrix if available)\n",
    "try:\n",
    "    is_top_1000_mask = build_top1000_mask_from_ltr(ltr_df, returns_matrix)\n",
    "except NameError:\n",
    "    # Fallback if returns_matrix is not in scope; still builds the mask from ltr_df only\n",
    "    is_top_1000_mask = build_top1000_mask_from_ltr(ltr_df, None)\n",
    "\n",
    "\n",
    "if 'mask_for_stocks_to_trade' in globals():\n",
    "    try:\n",
    "        aligned_ref = mask_for_stocks_to_trade.reindex_like(is_top_1000_mask).fillna(False).astype('bool')\n",
    "        diff = (is_top_1000_mask != aligned_ref)\n",
    "        n_diff = int(diff.to_numpy().sum())\n",
    "        if n_diff > 0:\n",
    "            # Print a short diagnostic so you can decide whether to harmonize the definitions.\n",
    "            n_total = is_top_1000_mask.size\n",
    "            frac = n_diff / n_total\n",
    "            print(f\"[Top-1000 mask] Warning: {n_diff:,} cells differ from mask_for_stocks_to_trade \"\n",
    "                  f\"({frac:.6%} of the matrix). This can happen if ltr_df applies extra row screens.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Top-1000 mask] Comparison to mask_for_stocks_to_trade skipped: {e}\")\n",
    "\n",
    "is_top_1000_mask.to_hdf(\n",
    "    \"features/is_top_1000_mask.h5\",\n",
    "    key=\"is_top_1000_mask\",\n",
    "    mode=\"w\",\n",
    "    complib=\"zlib\",\n",
    "    complevel=9\n",
    ")\n",
    "\n",
    "print(is_top_1000_mask.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
