{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bocd_features = pd.read_hdf(\"D:/oxford/Dissertation/cs_strats/features/bocd_feat_backups/bocd_per_stock_mle_asof_top1k_more_feats_v2_hazard_100_days.h5\", \n",
    "                            key='bocd_per_stock_mle_asof_top1k_more_feats_v2_hazard_100_days')\n",
    "\n",
    "bocd_features = pd.read_hdf(\"features/bocd_per_stock_feats.h5\", \n",
    "                            key='bocd_per_stock_feats')\n",
    "bocd_features['permno'] = bocd_features['permno'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_df = pd.read_hdf(\"features/ltr_df.h5\", 'ltr_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "433a0bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_df_w_bocd_feat = pd.merge(\n",
    "    ltr_df,\n",
    "    bocd_features,\n",
    "    how='left',\n",
    "    on=['date', 'permno']\n",
    ")\n",
    "\n",
    "ltr_df_w_bocd_feat.dropna(inplace=True)\n",
    "ltr_df_w_bocd_feat.columns = ltr_df_w_bocd_feat.columns.str.replace(\n",
    "    r'^P_r_le_', 'Pr_le_', regex=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca123d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save memoery\n",
    "del ltr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b234b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_section_winsor_z(df, feature_cols, date_col='date',\n",
    "                           lower_q=0.01, upper_q=0.99, eps=1e-8):\n",
    "    df = df.copy()\n",
    "    for f in feature_cols:\n",
    "        # Apply per date\n",
    "        grouped = df.groupby(date_col)[f]\n",
    "        # Compute quantiles\n",
    "        q_low  = grouped.transform(lambda x: x.quantile(lower_q))\n",
    "        q_high = grouped.transform(lambda x: x.quantile(upper_q))\n",
    "        clipped = df[f].clip(q_low, q_high)\n",
    "        mean = clipped.groupby(df[date_col]).transform('mean')\n",
    "        std  = clipped.groupby(df[date_col]).transform('std').replace(0, np.nan)\n",
    "        df[f + '_z'] = (clipped - mean) / (std + eps)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23629d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr_df_w_bocd_feat = cross_section_winsor_z(ltr_df_w_bocd_feat, \n",
    "                                            ['p_t', 'E_rt', 'dE_rt', 'Pr_le_5', 'Pr_le_10',\n",
    "                                             'Pr_le_14', 'Pr_le_21', 'Pr_le_42', 'Pr_le_63', 'Pr_le_126',\n",
    "                                             'Pr_le_252', 'log_pred', 'H_rt', 'Var_rt', 'r_med'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a65d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: THIS OVERWRITES THE EXISTING ltr_hdf, hence it is commented out (uncomment out after you verify it is safe)\n",
    "ltr_df_w_bocd_feat.to_hdf(\n",
    "    'features/ltr_df.h5',\n",
    "    key='ltr_df',\n",
    "    mode='w',\n",
    "    complib='blosc',\n",
    "    complevel=9\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
